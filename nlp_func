import os
import pandas as pd
import jieba
import re

import numpy
from sklearn.preprocessing import StandardScaler


import os
#from pyltp import SentenceSplitter, Segmentor, Postagger, Parser, NamedEntityRecognizer, SementicRoleLabeller
import csv

import jieba
import sys
import os
#from config_ch import *
import chardet
import numpy as np
import pandas as pd
import xlrd
import copy
import glob
import jieba.posseg
import jieba.analyse
import io
from sklearn import feature_extraction
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

os.getcwd()
## CHANGE
p = r'C:\Users\base1001\Documents\Python Scripts'
os.chdir(p)

pd.set_option('display.max_rows', 310)
pd.set_option('display.max_columns', 310)
pd.set_option('display.width', 500)

from round1_func import *
from nlp_func1 import *





def cut_word(word):
    #cw = jieba.cut_for_search(word)
    cw = jieba.cut(word)
    return list(cw)

def string_clean(l):
    l = str(l).replace("[", "")
    l = l.replace("]", "")
    l = l.replace("', '", "/")
    l = l.replace("@", "")
    l = l.replace("(", "")
    l = l.replace(")", "")
    l = l.replace("#", "")
    l = l.replace('【领券满99减20】', "")
    l = l.replace('【领券满39减8】', "")
    l = l.replace('【领券满89减8】', "")
    l = l.replace('【闪购福利】', "")
    l = l.replace('【泡面】', "")
    l = l.replace('【碗】', "")
    l = l.replace('【整箱】', "")
    l = l.replace('【整包】', "")
    l = l.replace('【大包】', "")
    l = l.replace('【单包】', "")
    l = l.replace('【五连包】', "")
    l = l.replace('【网红推荐】', "")
    l = l.replace('【网红爆款】', "")
    l = l.replace('【抖音爆款】', "")
    l = l.replace('【网红】', "")
    l = l.replace("【", "")
    l = l.replace("】", "")
    l = l.replace("亚州", "亚洲")
    l = l.replace("$",'')
    l = l.replace("*",'')
    l = l.replace("/",'')
    l = l.replace("@@",'')
    l = l.replace(" ",'')
    l = l.replace("（",'')
    l = l.replace("）",'')
    l = l.replace(".",'')
    l = l.replace("^",'')
    l = l.replace("_",'')
    l = l.replace("?",'')
    l = l.replace("4合1",'四合一')
    l = l.replace("3合1",'三合一')
    l = l.replace("2合1",'二合一')
    l = l.replace("1+2",'雀巢一加二')
    l = l.replace("3点1刻",'三点一刻')
    l = l.replace("3点一刻",'三点一刻')
    return l

def brand_clean(l):
    l = l.replace("亚州", "亚洲")
    l = l.replace("【", "")
    l = l.replace("】", "")
    l = l.replace("@@",'')
    l = l.replace("@", "")
    l = l.replace("(", "")
    l = l.replace(")", "")
    l = l.replace("#", "")
    l = l.replace("{", "")
    l = l.replace("}", "")
    l = l.replace("$",'')
    l = l.replace("*",'')
    l = l.replace("/",';')  
    return(l)


def debrand(l):
    l = str(l[0]).replace(str(l[1]), '')
    return l


import jieba
import jieba.analyse as analyse


import random
def extract_nr(text):
    allow_pos = ('nr','nz','nt','ns')
#allow_pos = ('nr','nz','nt', 'n')
    tags = analyse.extract_tags(text, topK=10, withWeight=False, allowPOS=allow_pos)
    return tags

def extract_a(text):
    allow_pos = ('a','ad','an','d')
#allow_pos = ('nr','nz','nt', 'n')
    tags = analyse.extract_tags(text, topK=10, withWeight=False, allowPOS=allow_pos)
    return tags


def extract_n(text):
    allow_pos = ('n','vn')
#allow_pos = ('nr','nz','nt', 'n')
    tags = analyse.extract_tags(text, topK=10, withWeight=False, allowPOS=allow_pos)
    return tags

import re
def brand_sp(text):
    UNESCAPED_SLASH_RE = re.compile(';|/')
    a = UNESCAPED_SLASH_RE.split(text)
    return a

def get_TF(words,topk = 5):
	
	tf_dic = {}
	for w in words:
		tf_dic[w] = tf_dic.get(w,0) + 1
	return sorted(tf_dic.items(),key = lambda x : x[1],reverse = True)[:topk]  #get top 10

def depart(s):
    eng = "".join(i for i in s if ord(i) < 256)
    chn = "".join(i for i in s if ord(i) >= 256)
    chn = chn.replace('包装','')
    chn = chn.replace('装','')
    return chn

def denumeric(s):
    result = ''.join([i for i in s if not i.isdigit()])
    return result


def text_prep(df, col='DESCRIPTION'):
    df[col]= df[col].apply(string_clean)
    df['textual'] = df[col].str.replace('\d+', '')
    df['textual'] = df['textual'].apply(depart)
    df['special'] =df['textual'].apply(extract_nr)
    df['nouns'] =df['textual'].apply(extract_n)
    df['ads'] =df['textual'].apply(extract_a)
    df['cut_DESCRIPTION']= eric_cat['textual'].apply(cut_word)
    
    return df
def numsplit(s):
    head = s.rstrip('0123456789')
    tail = s[len(head):]
    return [head, tail]

import os

import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

import tensorflow_hub as hub
import tensorflow_datasets as tfds
tfds.disable_progress_bar()

from official.modeling import tf_utils
from official import nlp
from official.nlp import bert

# Load the required submodules
import official.nlp.optimization
import official.nlp.bert.bert_models
import official.nlp.bert.configs
import official.nlp.bert.run_classifier
import official.nlp.bert.tokenization
import official.nlp.data.classifier_data_lib
import official.nlp.modeling.losses
import official.nlp.modeling.models
import official.nlp.modeling.networks

gs_folder_bert = "gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12"
tf.io.gfile.listdir(gs_folder_bert)


hub_url_bert = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"

glue, info = tfds.load('glue/mrpc', with_info=True,
                       # It's small, load the whole dataset
                       batch_size=-1)

info.features['label'].names
glue_train = glue['train']

for key, value in glue_train.items():
  print(f"{key:9s}: {value[0].numpy()}")

tokenizer = bert.tokenization.FullTokenizer(
    vocab_file=os.path.join(gs_folder_bert, "vocab.txt"),
     do_lower_case=True)

print("Vocab size:", len(tokenizer.vocab))

tokens = tokenizer.tokenize("Hello TensorFlow!")
print(tokens)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])

def encode_sentence(s):
   tokens = list(tokenizer.tokenize(s.numpy()))
   tokens.append('[SEP]')
   return tokenizer.convert_tokens_to_ids(tokens)

sentence1 = tf.ragged.constant([
    encode_sentence(s) for s in glue_train["sentence1"]])
sentence2 = tf.ragged.constant([
    encode_sentence(s) for s in glue_train["sentence2"]])

print("Sentence1 shape:", sentence1.shape.as_list())
print("Sentence2 shape:", sentence2.shape.as_list())

cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]
input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)

input_mask = tf.ones_like(input_word_ids).to_tensor()

type_cls = tf.zeros_like(cls)
type_s1 = tf.zeros_like(sentence1)
type_s2 = tf.ones_like(sentence2)
input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()

plt.pcolormesh(input_type_ids)

plt.pcolormesh(input_mask)
_ = plt.pcolormesh(input_word_ids.to_tensor())

def encode_sentence(s, tokenizer):
   tokens = list(tokenizer.tokenize(s))
   tokens.append('[SEP]')
   return tokenizer.convert_tokens_to_ids(tokens)

def bert_encode(glue_dict, tokenizer):
  num_examples = len(glue_dict["sentence1"])

  sentence1 = tf.ragged.constant([
      encode_sentence(s, tokenizer)
      for s in np.array(glue_dict["sentence1"])])
  sentence2 = tf.ragged.constant([
      encode_sentence(s, tokenizer)
       for s in np.array(glue_dict["sentence2"])])

  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]
  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)

  input_mask = tf.ones_like(input_word_ids).to_tensor()

  type_cls = tf.zeros_like(cls)
  type_s1 = tf.zeros_like(sentence1)
  type_s2 = tf.ones_like(sentence2)
  input_type_ids = tf.concat(
      [type_cls, type_s1, type_s2], axis=-1).to_tensor()

  inputs = {
      'input_word_ids': input_word_ids.to_tensor(),
      'input_mask': input_mask,
      'input_type_ids': input_type_ids}

  return inputs
